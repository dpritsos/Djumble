\documentclass[dvips,dvipdfm,pdftex]{llncs}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[dvips,dvipdfm,pdftex]{graphicx}
\usepackage{pgfplotstable}
\usepackage{array}
\usepackage{booktabs}
\usepackage{morefloats}
\usepackage{etex}
\usepackage{listings}
\usepackage{float}
\usepackage[section]{placeins}
\usepackage[ruled,linesnumbered,resetcount,algochapter]{algorithm2e}


\title{Semi-supervised Hidden Markov Random Fields (HMRF) Kmeans. Theory to Implementation\\}
\author{Dimitrios Pritsos and Efstathios Stamatatos}
\institute{University of the Aegean\\Karlovassi, Samos \textendash{} 83200, Greece.\\\{dpritsos, stamatatos\}@aegean.gr }

\begin{document}

\maketitle

\begin{abstrat}

\end{abstrat}


\section{Introduction}\label{sec:intro}
The objective of the Semi-supervised Clustering is to incorporate in the procedure of clusters discovery or assignment, the prior knowledge about the skeleton of the clusters schema. There are several efforts on Semi-supervised model inference in both Expectation Maximization (EM) clustering based models and in Agglomerating clustering based models. According to \cite{chapelle2006semi_hmrf_kmeans} there three main groups of EM based semi-supervised clustering methods:

\begin{enumerate}
	\item \emph{Constraints-based mehtods} are using the provided supervision for guiding the algorithm towards a data partitioning which is avoiding (but not prevening) the constraints violation.
	\item \emph{Distance-based approaches} in clustering method with a particular distance funciton; the distance function is parametrized and the parameters values are learned to satisfy the constraints.
	\item \emph{Semi-supervised clustering based on Hidden Markov Random Fields} where the constraint-based and distance-based approaches are combined into \emph{a unified probabilistic model}.
\end{enumerate}

In EM clustering based models there have been several efforts where the labeled data where provided in the clustering model in the form of data-labels pairs or in the initialization phase of theclustering model. In this work we present the Hidden Markov Random Fields Model(HMRF) Kmeans, where the prior knowledge about the structure or skeleton of the clusters schema has been embedded into the model in the form of constraints \cite{basu2004probabilistic}. The HMRF Kmeans is a hard-clustering model due to the \emph{hard} assignment of the data point to one of the a-priori fixednumber of clusters. However, the same models can be transformed into a relatively easy soft-clustering model where of each data point only Maximum a-posteriry Probability (MAP) of the point to bea member of each cluster of th final schema.

The HMRF Kmeans Semi-supervised clustering method it has been implemented, in this work, for being tested on the Web Genre Identification (WGI) Information Retrieval (IR) taxonomy problem.Therefore, here we only present the model inference procedure where the distance measure, a.k.a distortion function/measure, is the cosine similarity because in the IR literature is the distortionmeasure where in most cases maximizes performance in problems where the feature space is particularly large, as in this case. In case one would be interested in changing the model to asoft-clustering method then the probability destiny function of the final model should have been chosen to be some properly parametrized Von Mises Fisher distributions.

Since this work is focusing on the implementation of Semi-supervised HMRF-Kmeans in the IR domain framework, it has to be noted that this Semi-supervised model advantage is the interactivelearning setting where this model can be used \cite{chapelle2006semi_hmrf_kmeans}, since the constraints are provided to the model in two different sets the \emph{Must-Link} and\emph{Cannot-Link}. These sets are not necessarily the same in size or complimentary one to the other.

What it follows is the model inference line of thought based on the there resources \cite{basu2004probabilistic,chapelle2006semi_hmrf_kmeans,bishop2006_EM_general_view}.

\section{Model inference}\label{sec:model_inference}
The objective of a semi-supervised model like HMRF Kmeans is to drive the procedure of clustering schema taking into account the prior knowledge we have about the clusters in the form of\emph{must-link} and \emph{cannot-link} constraints. The main difference in the graphical model of a topical EM algorithm is the nodes of the observed labeled data over the hidden, a.k.a latent,variables as depicted in fig.\ref{fig:hmrf_em}, with red colored arrows and gray shaded nodes.

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.40]{figures/SemiSupervised_HMRF_EM_Graphical_Model.eps}
		\caption{Semi-supervised HMRF Expectation Maximization or Kmeans Clustering Graphical Model.}
		\label{fig:hmrf_em}
	\end{center}
\end{figure}

The goal of EM is to maximize the \emph{log likelihood function} $P(X|\Theta)$ with respect of $\Theta$ as in eq.\ref{eq:em_log_likelihood}.

\begin{equation}
lnP(X|\Theta)=ln\left\{ \sum_{l}P(X,L|\Theta)\right\}
\label{eq:em_log_likelihood}
\end{equation}

Where $X=\{\mathbf{x}_{i}\}_{i=1}^{N}$ is the set of \emph{observable random variables} given by the conditional probability $P(X|\Theta)$ and $\mathbf{x}_{i}$ is a random vector (or data point) of the corpus under taxonomy. Note the boldface notation of the random vector in order not to be mixed with $x_{i}$ which will a feature (or variable) of this vector. Moreover, $N$ is the number of vector as depicted in the graphical model of fig.\ref{fig:hmrf_em}.

Due to the relatively complex marginal distributions, like $P(X|\Theta)$, over observed data points where they are computationally intractable, there is a common practice to incorporate \emph{latent or hidden variables} in order to express the conditional probability calculation more tractable over the expanded space of observed and latent variables. In eq.\ref{eq:em_log_likelihood} $L$ is the set of hidden (or latent) variables over $X$ observable data points \cite{bishop2006_EM_general_view}.

Therefor \emph{a hidden field} $L=\{\mathbf{l}_{i}\}_{i=1}^{N}$ random variables, \emph{whose values are unobservable}. In the clustering framework, the set of hidden variables are the unobserved cluster labels on the
points, indicating cluster assignments. Every hidden variable $\mathbf{l}_{i}$ takes values from the set ${1, . . . , N}$, which are the labels of the clusters.

Now every random data point $\mathbf{x}$ can be generated from a conditional probability distribution $P(\mathfb{x}_{i}|\mathbf{l}_{i})$ determined by the corresponding hidden variable $\mathbf{l}_{i}$. Note that we know a-priory that the random data points $X$ are conditionally independent given the hidden variables $L$. Thus $P(X|L) = \prod_{\mathbf{x}_{i} \in X}P(\mathbf{x}_{i}|\mathbf{l}_{i})$. Note that $\mathbf{l}_{i}$ can be either a vector or a singleton depending on the algorithm setting, either for soft-clustering or for hard-clustering respectively. Thus value and vector for $\mathbf{l}_{i}$ will be used interchangeably in this text.

The set of $\left\{X,Z\right\}$ is \emph{the complete data set}, while the $\left\{X\right\}$ is \epmh{the incomplete data set}. The likelihood function for the complete data set simply takes the form $lnp(X,Z|\Theta)$ as shown in eq.\ref{eq:em_log_likelihood}, and theoretically that maximization of this complete data log likelihood function is straightforward. However, in practice we are not given the complete data set, but only the incomplete data points $X$. Thus, our only knowledge of for the values of the latent (hidden) variables in $L$ is given by the posterior distribution $p(Z|X,\Theta)$. Because we don't have available the complete-data log likelihood, we consider instead its expected value under the posterior distribution of the latent (hidden) variables.

In eq.\ref{eq:em_log_likelihood} and in fig.\ref{fig:hmrf_em} there are some parameters $\Theta$ which are governing the initial and the final schema of the PDF's mixture. These are the parameters we have to find computationally and where in employee EM algorithm (alg.\ref{alg:em_generic}) with the following general and distribution inexpedient (i.e. irrespectively where the PDF's are Gaussian, Von Mises Fisher etc). In particular with EM we are interactively finding the proper set of $\Theta$ parameters by calculating the expected posterior distribution $P(L|X,\Theta)$ of the latent (hidden) variables $L$.

\begin{algorithm}[H]\label{alg:em_generic}
\KwData{\\
		\hspace{.01\textwidth} $\mathbf{X}$ observable data points.\\
		\hspace{.01\textwidth} $\mathbf{K}$ possible states clusters we expect to be existing in the data-set.\\
		\hspace{.01\textwidth} $\mathbf{L}_{i=1}^K$ the hidden field variables (or vectors) with values $\left\{1,...,\mathbf{K}\right\}$ or $(0,1]$\\
		\hspace{.01\textwidth} $\mathbf{\Theta}$ an initial state about the PDF mixture model.\\
		}
\KwResult{\\
		  \hspace{.01\textwidth} $\mathbf{l} = \{\mathbf{p}_{k}\}_{k=1}^{K}$ where $\mathbf{p}$ can be either $\left\{0,1\right\}$ or $(0,1]$
 		  depending on the soft-clustering of hard-clustering set-up.\\
		  \hspace{.01\textwidth} $\mathbf{\Theta}$ the final set of parameters after the Probability Density Functions Mixture.
		 }
Choose an Initial setting for $\mathbf{\Theta}^{OLD}$;\\
\While{$I$ iterations reached}{
	\textbf{1. E-step }Evaluate $P(\mathbf{L}|\mathbf{X},\mathbf{\Theta}^{OLD})$\;\\
	\textbf{2. M-step }Evaluate $\mathbf{\Theta^{NEW}}$\ given by (a) and (b);\\
	\hspace{.1\textwidth} \textbf{a.} $\mathbf{\Theta}^{NEW}=\arg_{\mathbf{\Theta}}\max\Omega(\mathbf{\Theta},\mathbf{\Theta}^{OLD})$\;\\
	\hspace{.1\textwidth} \textbf{b.} $\Omega(\mathbf{\Theta},\mathbf{\Theta}^{OLD})=\sum_{\mathbf{L}}P(\mathbf{L|X},\mathbf{\Theta}^{OLD})lnP(\mathbf{X,L},\mathbf{\Theta})$\;\\

	\eIf{log likelihood convergence reached}{
		Breaking the loop and Ending the Clustering\;
	}{
		$\mathbf{\Theta}^{OLD}\leftarrow\mathbf{\Theta}$\;
	}
}

\caption{The generic for of Expectation Maximization either for both Soft- and Hard-clustering }
\end{algorithm}

The EM algorithm can also be used to find MAP (maximum a-posterior) solutions in case we have a good knowledge about the prior distribution $P(\mathbf{\Theta})$ over the parameters $\mathbf{\Theta}$. In this case the E-step remains the same as in the maximum likelihood case, while in the \emph{M-step (b)} the $\Omega(\mathbf{\Theta},\mathbf{\Theta}^{OLD})+lnP(\mathbf{\Theta})$.

\textbf{In HMRF-Kmeans derivation process} we are starting with the PDF mixture we would like to maximize by exploring the EM for the reasons explained above, as shown in eq.\ref{eq:hmrm_em_pdf}.

\begin{equation}
	P(\mathbf{X,L,\Theta}|\mathbf{M,C})=P(\mathbf{\Theta}|\mathbf{M,C})P(\mathbf{L}|\mathbf{\Theta,M,L})P(\mathbf{X}|\mathbf{L,\Theta,M,C})
\label{eq:hmrm_em_pdf}
\end{equation}

Where the set of vector are the same as in alg.\ref{alg:em_generic} but this time the constraints set of the fig.\ref{fig:hmrf_em} have been included, i.e. Mast-link and Cannot-link constraints set $\left\{\mathbf{M,C}\right\}$. Moreover, as the graphical model is describing the constraints are independent from $\mathbf{X}$ and parameters $\mathbf{\Theta}$ are, also, independent from the constraints set. Thus:

\begin{equation}
	P(\mathbf{L}|\mathbf{\Theta,M,L})P(\mathbf{X}|\mathbf{L,\Theta,M,C})=P(\mathbf{X}|\mathbf{L,M,C})
\label{eq:hmrm_em_X_MC_interdependency}
\end{equation}

\begin{equation}
	P(\mathbf{\Theta}|\mathbf{M,C})=P(\mathbf{\Theta})
\label{eq:hmrm_em_Theta_MC_interdependency}
\end{equation}

Considering $\mathbf{X}$ observable data-set is convient due to compuational issues to simplify the algorithm by assuming the vectors $\mathbf{x}$ are \emph{mutually impediment}, thus:

\begin{equation}
	P(\mathbf{X}|\mathbf{L,\Theta})=\prod_{i=1}^{N}P(\mathbf{x}_{i}|\mathbf{l}_{i},\mathbf{M,L})
\label{eq:hmrm_em_X_mutually_interdependency}
\end{equation}

Consequentially, from equations \ref{eq:hmrm_em_pdf}, \ref{eq:hmrm_em_X_MC_interdependency}, \ref{eq:hmrm_em_Theta_MC_interdependency} and \ref{eq:hmrm_em_X_mutually_interdependency} we are getting the following MAP which it should be maximize.

\begin{equation}
	P(\mathbf{X,L,\Theta}|\mathbf{M,C})=P(\mathbf{\Theta})P(\mathbf{L}|\mathbf{\Theta,M,C})\prod_{i=1}^{N}P(\mathbf{x}_{i}|\mathbf{l}_{i},\mathbf{M,L})
\label{eq:hmrm_em_MAP_or_MAX_Liklihood}
\end{equation}

At this step of the clustering method building process we have to decide weather the clustering would be soft or hard. This decision has two consequences. Firstly, is related to the $\mathbf{L}$ latent variables type and range of values as shown in alg.\ref{alg:em_generic}, i.e. whether $\mathbf{l}$ will be a variable or a vector of variables and whether its values will be probability estimates or {0,1} depending whether of not a data-point is belonging to the cluster $\mathbf{k}_{i}$. Secondly, is related whether the algorithm will be probabilistic based or distance based, i.e. whether a MAP will be maximized or an objective function with a specific distance measure (a.k.a distortion function/measure) will be minimized.

In our case go for the \emph{distortion function} path, as the Kmeans term of the algorithm implies. As explained above since we are focusing on IR domain problems we are going to show the complete algorithm building process for the cosine similarity as the distortion function of our choice.

Each hidden random variable $\mathbf{l}_i$ has an associated set of neighbors $\mathbf{\Gamma}_{i}\subset\mathbf{L}$. The must-link and cannot-link constraints define the neighborhood over the hidden labels, such that the neighbors of a point $x_{i}$ are all points that must-linked and/or cannot-inked with. The \emph{random field defined over the hidden variables} is a \emph{Markov Random Field}, where the PDF of the hidden variables \emph{obeys the following Markov property}:

\begin{equation}
	\forall i,P(\mathbf{l}_{i}|\mathbf{L-}\{\mathbf{l}_{i}\},\mathbf{\Theta},\mathbf{M},\mathbf{C})=P(\mathbf{l}_{i}|\{\mathbf{\Gamma}_{i},\mathbf{\Theta},\mathbf{M},\mathbf{C}\})
\label{eq:markov_property}
\end{equation}

Therefore, the probability distribution of $\mathbf{l}$ labels is only dependent on the must-link and cannot-link constraints on $\mathbf{x}$ data-points. The above paragraph justifies the name for the algorithm as HMRF and letting us assume any a-piory PDF for an arbitrary label setup.

Since, by the \emph{Hammersley-Clifford theorem}, the a-prior PDF of a particular label setup can be expressed as a Gibbs distribution eq.\ref{eq:constraints_Gibbs_pdf}, which conveniently belongs to the exponential distributions family, as we will see later.

\begin{equation}
	P(\mathbf{L}|\mathbf{\Theta},\mathbf{M},\mathbf{C})=\frac{1}{Z_{1}}\exp(-\sum_{\mathbf{\Gamma}_{i}\in\Gamma}V_{\Gamma_{i}}(\mathbf{L}))
\label{eq:constraints_Gibbs_pdf}
\end{equation}

Where $V_{\Gamma_{i}}$ is the \emph{potential function}, as it called when the clustering model is \emph{hard} type \textbf{(NOTE: Potential Function also found this in the Bishops Book and I think this term is used in Kmeans case and not in the probabilistic EM)}, for each neighborhood of labels defined by the $\{M,C\}$ sets of constraints.

Since the constraints should probably been given as pairs then eq.\ref{eq:constraints_Gibbs_pdf} will become eq.\ref{eq:constraints_Gibbs_pdf_pairwise}.\textbf{(NOTE:A question here is constraint A=(2,50) will be the same as B=(50,2), will both be present etc. Should the constraints be a Graph, a Matrix or a set of pairs? (Impementation Language Performance VS Math/Computational Performance))}.

\begin{equation}
	P(\mathbf{L}|\mathbf{\Theta},\mathbf{M},\mathbf{C})=\frac{1}{Z_{1}}\exp(-\sum_{i}\sum_{j}V(i,j))
\label{eq:constraints_Gibbs_pdf_pairwise}
\end{equation}

\begin{equation}
	V(i,j)=
	\begin{cases}
		f_{\mathbf{M}}(x_{i},x_{j}) & \text{if }(x_{i},x_{j})\in\mathbf{M}\\
		f_{\mathbf{C}}(x_{i},x_{j}) & \text{if }(x_{i},x_{j})\in\mathbf{C}\\
		0 & \text{otherwise}
	\end{cases}
\label{eq:constraints_potential_function}
\end{equation}

Where $f_{\mathbf{M}}$ and $f_{\mathbf{C}}$ are positive value cost functions for the must-link and cannot-link constraints violation. Clearly, eq.\ref{eq:constraints_Gibbs_pdf_pairwise} is discouraging the violation of the constraints by reducing the value of joint probability MAP in eq.\ref{eq:hmrm_em_MAP_or_MAX_Liklihood}.

Maximizing the joint HMRF probability in eq.\ref{eq:hmrm_em_MAP_or_MAX_Liklihood} (left part of the equation), is equivalent to jointly maximizing the likelihood of generating data points from the model and the probability of label assignments that respect the constraints, while regularizing the model parameters. The essential part of the right side of the equation the conditional probability $P(\mathbf{X}|\mathbf{L},\mathbf{M},\mathbf{C})$, equivalent to eq.\ref{eq:hmrm_em_X_MC_interdependency}. Firstly, because its PDF will define our prior assumption about the PDF mixture of the data-set. Secondly, because the same PDF should be used as the constraints potential function $V$, thus, penalty functions $f$. Finally, because its PDF will ultimately define the parametrized (by $\Theta$ parameters) \emph{distortion measure}.

Using the convenience in the assumption of a regular exponential distributions for the observed data points $X$ and a regular Bregman divergences, the PDF of the observed data would be in eq.\ref{eq:PDF_with_distortion_measure}.

\begin{equation}
	P(\mathbf{x}_{i}|\mathbf{l}_{i},\mathbf{\Theta})=\frac{1}{Z_{2}}\exp(-D_{\mathbf{A}}(x_{i},\mathbf{\mu}_{\mathbf{l}_{i}}))
\label{eq:PDF_with_distortion_measure}
\end{equation}

where $D_{\mathbf{A}}(\mathbf{x}_{i} , \mathbf{x}_{\mathbf{l}_{i}})$ is the Bregman divergence, i.e. distortion measure, between $\mathbf{x}_i$ and $\mathbf{\mu}_{\mathbf{l}_{i}}$. Where $\mathbf{l}_i$ is indicating that the distance is measured for an arbitrary $\mathbf{x}_{i}$ is from the cluster centroid (or cluster's PDF expected value) when the point and the centroid a under the same cluster PDF \textbf{(NOTE: This is my conclusion and no one in literature give any specification related to the $\mu$ notation)}. $Z_{2}$ is the normalizing term (a.k.a \emph{partition function}).

As explained above in general EM algorithm description, we are at the step where we have to assume the PDF mixture of the observable data. In the semi-supervised case the a-piori distribution assumption will be the same, over the observable data-set $\mathbf{X}$ and the observable constraints sets $\mathbf{M,C}$. Thus, in our case, for the IR problems based on the literature, we assume for both the data-set and the constraints sets von-Mises Fisher (vMF) distribution with unit concentration parameter, which is equivalent to the spherical Gaussian distribution \cite{basu2004probabilistic,chapelle2006semi_hmrf_kmeans}. In order to make PDF of eq.\ref{eq:PDF_with_distortion_measure} to be equivalent to vMF the distortion parameter $D_{\mathbf{A}}$ will be replaced with the \emph{parametrized cosine distance} as defined in eq.\ref{eq:parametrized_cos_sim} by the parameters matrix $\mathbf{A}$.

\begin{equation}
	D_{cos_{\mathbf{A}}}(\mathbf{x}_{i},\mathbf{x}_{j})=\frac{\mathbf{x}_{i}^{T}\mathbf{A}\mathbf{x}_{j}}{\Vert\mathbf{x}_{i}\Vert_{\mathbf{A}}\Vert\mathbf{x}_{j}\Vert_{\mathbf{A}}}
	\label{eq:parametrized_cos_sim}
\end{equation}

Where $\Vert\mathbf{x}_{i}\Vert_{\mathbf{A}}=\sqrt{\mathbf{\Vert x}_{i}^{T}\mathbf{A}\mathbf{x}_{j}\Vert}$, \textbf{(NOTE: In \cite{basu2004probabilistic} under square root there is no norm, but in practice the product sometimes drops under zero and breaks the code run.)}

In the eq.\ref{eq:hmrm_em_MAP_or_MAX_Liklihood} (right side of the equation) we have defined the conditional probabilities for the observed $X$ and the $L$, where the second is conditioned under the constraints ${M,C}$. The presence of the $P(\mathbf{\Theta})$ is occurring due to the explicit presence of the joint distribution in eq.\ref{eq:hmrm_em_MAP_or_MAX_Liklihood} (left side of the equation) and the graphical model in fig.\ref{fig:hmrf_em}. This is enabling us to decide whether or not we want to apply our a-piori knowledge about the distribution of the parameters. This can be omitted when $\mathbf{\Theta}$ parameters are not explicitly present, as in \cite{basu2004probabilistic}, thus no prior assumptions about the parameters distribution is required or can be applied.

Depending on the prior distribution, parameters $\mathbf{\Theta}$ are separated in two sets $\mathbf{\Theta}=\{\mathbf{A},\mathbf{M}\}$, where the second one is the expected values of the mixture's PDF(s) and $\mathbf{A}$ is the rest of the required parameters.

\begin{equation}
	P(\mathbf{\Theta})=P(\mathbf{A})P(\mathbf{Mu})
\label{eq:ptheta}
\end{equation}


\subsection{Putting all together}

The objective of the semi-supervised HMRF Kmeans is to maximize the MAP as defined in eq.\ref{eq:hmrm_em_MAP_or_MAX_Liklihood} with the parts of this equation to be analyses in eq.\ref{eq:constraints_Gibbs_pdf} eq.\ref{eq:constraints_Gibbs_pdf} and eq.\ref{eq:ptheta} . Putting all these equations together we have eq.\ref{eq:final_MAP}.

\begin{equation}
	P(\mathbf{X,L,\Theta}|\mathbf{M,C})=P(\mathbf{A})P(\mathbf{Mu})(\frac{1}{Z_{1}}\exp(-\sum_{\mathbf{\Gamma}_{i}\in\Gamma}V_{\Gamma_{i}}(\mathbf{L})))\prod_{i=1}^{N}\frac{1}{Z_{2}}\exp(-D_{\mathbf{A}}(x_{i},\mathbf{\mu}_{\mathbf{l}_{i}}))
\label{eq:final_MAP}
\end{equation}


Our early decision was the hard-clustering setup for the algorithm thus my explaining the logarithmic properties from eq.\ref{eq:final_MAP} we are getting eq.\ref{eq:MAP_to_Objective}.

\begin{equation}
	lnP(\mathbf{X,L,\Theta}|\mathbf{M,C})=lnP(\mathbf{A})+lnP(\mathbf{Mu})-\sum_{\mathbf{\Gamma}_{i}\in\Gamma}V_{\Gamma_{i}}(\mathbf{L})+\sum_{i=1}^{N}D_{\mathbf{A}}(x_{i},\mathbf{\mu}_{\mathbf{l}_{i}})+lnZ_{1}+NlnZ_{2}
\label{eq:MAP_to_Objective}
\end{equation}

Equation eq.\ref{eq:MAP_to_Objective} is now called the \emph{Objective Function} and the goal of the algorithm is to minimize its output, which is equivalent to the MAP maximization. Expressing it in words the minimization process of eq.\ref{eq:MAP_to_Objective} is equivalent to the the iterative process in order to find the proper clustering configuration where each arbitrary data-point is the closest to the mean value of the cluster it will belong into at the end. Moreover, the constraints violation will also be minimized.

Before we proceed in replacing the exact violation $V$ and distortion measure $D$ functions with the parameterized cosine similarity we are considering the rest of the equation starting with the normalizing components $Z_{1}$ and $Z_{2}$. In additions to consider the distributions of the PDF mixture parameters $A$ and $Mu$.

The normalizing components of observable data points and the hidden variables distributions are depended on the prior PDF assumption. In case of Gaussian ditriobution thier vaule can be calculated in a \emph{closed form} (NOTE:Sure excatly this is) while for all the other case we have to employee an \emph{approximation method}. In our case where cosine similarity is must be used an approximate inference method is required which can be very expensice coputationally. In this case we can assume that $Z$ and $Z$ are constant but not eq.\ref{eq:MAP_to_Objective} will no longer be an joint probablitiy. However, empirically it has been shown that this objective funcitons will work properly and the alogorithm will manage to convege in a local minimum \cite{basu2004probabilistic}. However, if in some applications it is important to preserve the semantics of the underlying joint probability model, then the normalizers Z and ZÎ˜ must be estimated by approximate inference methods.

 


\bibliographystyle{splncs03}
\bibliography{djumble_doc}

\end{document}
